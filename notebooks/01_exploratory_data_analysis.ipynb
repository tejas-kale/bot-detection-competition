{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis - Bot Detection Competition\n",
    "\n",
    "This notebook performs initial exploratory data analysis (EDA) on the bot detection competition dataset to understand the characteristics of human vs. AI-generated essays.\n",
    "\n",
    "## Objectives\n",
    "1. Load and examine the structure of the training data\n",
    "2. Analyze text length distributions\n",
    "3. Examine word count statistics\n",
    "4. Investigate punctuation usage patterns\n",
    "5. Assess class balance (human vs. AI)\n",
    "6. Generate initial hypotheses for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import textstat\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_essays = pd.read_csv('../data/train_essays.csv')\n",
    "train_prompts = pd.read_csv('../data/train_prompts.csv')\n",
    "sample_submission = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"Training essays: {train_essays.shape}\")\n",
    "print(f\"Training prompts: {train_prompts.shape}\")\n",
    "print(f\"Sample submission: {sample_submission.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the structure of training essays\n",
    "print(\"Training Essays Info:\")\n",
    "print(train_essays.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(train_essays.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine training prompts\n",
    "print(\"Training Prompts Info:\")\n",
    "print(train_prompts.info())\n",
    "print(\"\\nPrompt details:\")\n",
    "display(train_prompts[['prompt_id', 'prompt_name']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in training essays:\")\n",
    "print(train_essays.isnull().sum())\n",
    "print(\"\\nMissing values in training prompts:\")\n",
    "print(train_prompts.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Class Balance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution\n",
    "class_counts = train_essays['generated'].value_counts()\n",
    "class_proportions = train_essays['generated'].value_counts(normalize=True)\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "print(f\"Human-written (0): {class_counts[0]} ({class_proportions[0]:.2%})\")\n",
    "print(f\"AI-generated (1): {class_counts[1]} ({class_proportions[1]:.2%})\")\n",
    "\n",
    "# Visualize class balance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Bar plot\n",
    "class_counts.plot(kind='bar', ax=ax1, color=['skyblue', 'lightcoral'])\n",
    "ax1.set_title('Class Distribution (Count)')\n",
    "ax1.set_xlabel('Class (0=Human, 1=AI)')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "ax2.pie(class_counts.values, labels=['Human', 'AI'], autopct='%1.1f%%', \n",
    "        colors=['skyblue', 'lightcoral'])\n",
    "ax2.set_title('Class Distribution (Proportion)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text lengths\n",
    "train_essays['text_length'] = train_essays['text'].str.len()\n",
    "train_essays['word_count'] = train_essays['text'].str.split().str.len()\n",
    "train_essays['sentence_count'] = train_essays['text'].str.split('.').str.len() - 1\n",
    "\n",
    "# Basic statistics for text length by class\n",
    "text_stats = train_essays.groupby('generated')['text_length'].describe()\n",
    "print(\"Text Length Statistics by Class:\")\n",
    "print(text_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize text length distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Distribution by class\n",
    "for i, class_val in enumerate([0, 1]):\n",
    "    class_name = 'Human' if class_val == 0 else 'AI'\n",
    "    data = train_essays[train_essays['generated'] == class_val]['text_length']\n",
    "    \n",
    "    axes[0, i].hist(data, bins=50, alpha=0.7, color='skyblue' if class_val == 0 else 'lightcoral')\n",
    "    axes[0, i].set_title(f'Text Length Distribution - {class_name}')\n",
    "    axes[0, i].set_xlabel('Text Length (characters)')\n",
    "    axes[0, i].set_ylabel('Frequency')\n",
    "\n",
    "# Box plot comparison\n",
    "train_essays.boxplot(column='text_length', by='generated', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Text Length by Class (Box Plot)')\n",
    "axes[1, 0].set_xlabel('Class (0=Human, 1=AI)')\n",
    "\n",
    "# Violin plot\n",
    "sns.violinplot(data=train_essays, x='generated', y='text_length', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Text Length by Class (Violin Plot)')\n",
    "axes[1, 1].set_xlabel('Class (0=Human, 1=AI)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word Count Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count statistics by class\n",
    "word_stats = train_essays.groupby('generated')['word_count'].describe()\n",
    "print(\"Word Count Statistics by Class:\")\n",
    "print(word_stats)\n",
    "\n",
    "# Visualize word count distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "train_essays[train_essays['generated'] == 0]['word_count'].hist(bins=50, alpha=0.7, \n",
    "                                                                 label='Human', color='skyblue', ax=axes[0])\n",
    "train_essays[train_essays['generated'] == 1]['word_count'].hist(bins=50, alpha=0.7, \n",
    "                                                                 label='AI', color='lightcoral', ax=axes[0])\n",
    "axes[0].set_title('Word Count Distribution by Class')\n",
    "axes[0].set_xlabel('Word Count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "sns.boxplot(data=train_essays, x='generated', y='word_count', ax=axes[1])\n",
    "axes[1].set_title('Word Count by Class')\n",
    "axes[1].set_xlabel('Class (0=Human, 1=AI)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Punctuation Usage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_punctuation(text):\n",
    "    \"\"\"Analyze punctuation usage in text\"\"\"\n",
    "    punctuation_counts = {}\n",
    "    \n",
    "    # Count specific punctuation marks\n",
    "    punctuation_counts['periods'] = text.count('.')\n",
    "    punctuation_counts['commas'] = text.count(',')\n",
    "    punctuation_counts['exclamations'] = text.count('!')\n",
    "    punctuation_counts['questions'] = text.count('?')\n",
    "    punctuation_counts['semicolons'] = text.count(';')\n",
    "    punctuation_counts['colons'] = text.count(':')\n",
    "    punctuation_counts['quotations'] = text.count('\"') + text.count(\"'\")\n",
    "    punctuation_counts['parentheses'] = text.count('(') + text.count(')')\n",
    "    \n",
    "    # Total punctuation\n",
    "    total_punct = sum(1 for char in text if char in string.punctuation)\n",
    "    punctuation_counts['total_punctuation'] = total_punct\n",
    "    \n",
    "    # Punctuation density (per 100 characters)\n",
    "    text_len = len(text)\n",
    "    punctuation_counts['punct_density'] = (total_punct / text_len * 100) if text_len > 0 else 0\n",
    "    \n",
    "    return punctuation_counts\n",
    "\n",
    "# Apply punctuation analysis\n",
    "punct_data = train_essays['text'].apply(analyze_punctuation)\n",
    "punct_df = pd.DataFrame(list(punct_data))\n",
    "punct_df['generated'] = train_essays['generated'].values\n",
    "\n",
    "# Display punctuation statistics by class\n",
    "print(\"Punctuation Statistics by Class:\")\n",
    "punct_stats = punct_df.groupby('generated')[['periods', 'commas', 'exclamations', 'questions', \n",
    "                                           'punct_density']].mean()\n",
    "print(punct_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize punctuation usage\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Punctuation density\n",
    "sns.boxplot(data=punct_df, x='generated', y='punct_density', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Punctuation Density by Class')\n",
    "axes[0, 0].set_xlabel('Class (0=Human, 1=AI)')\n",
    "\n",
    "# Period usage\n",
    "sns.boxplot(data=punct_df, x='generated', y='periods', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Period Usage by Class')\n",
    "axes[0, 1].set_xlabel('Class (0=Human, 1=AI)')\n",
    "\n",
    "# Comma usage\n",
    "sns.boxplot(data=punct_df, x='generated', y='commas', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Comma Usage by Class')\n",
    "axes[1, 0].set_xlabel('Class (0=Human, 1=AI)')\n",
    "\n",
    "# Exclamation usage\n",
    "sns.boxplot(data=punct_df, x='generated', y='exclamations', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Exclamation Usage by Class')\n",
    "axes[1, 1].set_xlabel('Class (0=Human, 1=AI)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Readability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_readability_scores(text):\n",
    "    \"\"\"Calculate various readability scores for text\"\"\"\n",
    "    try:\n",
    "        scores = {\n",
    "            'flesch_kincaid': textstat.flesch_kincaid_grade(text),\n",
    "            'flesch_reading_ease': textstat.flesch_reading_ease(text),\n",
    "            'gunning_fog': textstat.gunning_fog(text),\n",
    "            'coleman_liau': textstat.coleman_liau_index(text),\n",
    "            'automated_readability': textstat.automated_readability_index(text),\n",
    "            'avg_sentence_length': textstat.avg_sentence_length(text),\n",
    "            'syllable_count': textstat.syllable_count(text),\n",
    "        }\n",
    "    except:\n",
    "        # Handle cases where text might be empty or cause errors\n",
    "        scores = {key: 0 for key in ['flesch_kincaid', 'flesch_reading_ease', 'gunning_fog', \n",
    "                                   'coleman_liau', 'automated_readability', 'avg_sentence_length', \n",
    "                                   'syllable_count']}\n",
    "    return scores\n",
    "\n",
    "# Calculate readability scores (sample first 1000 rows for faster computation)\n",
    "sample_size = min(1000, len(train_essays))\n",
    "sample_essays = train_essays.head(sample_size).copy()\n",
    "\n",
    "print(f\"Calculating readability scores for {sample_size} essays...\")\n",
    "readability_data = sample_essays['text'].apply(calculate_readability_scores)\n",
    "readability_df = pd.DataFrame(list(readability_data))\n",
    "readability_df['generated'] = sample_essays['generated'].values\n",
    "\n",
    "# Display readability statistics\n",
    "print(\"\\nReadability Statistics by Class:\")\n",
    "readability_stats = readability_df.groupby('generated')[['flesch_kincaid', 'flesch_reading_ease', \n",
    "                                                        'gunning_fog', 'avg_sentence_length']].mean()\n",
    "print(readability_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize readability scores\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Flesch-Kincaid Grade Level\n",
    "sns.boxplot(data=readability_df, x='generated', y='flesch_kincaid', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Flesch-Kincaid Grade Level by Class')\n",
    "axes[0, 0].set_xlabel('Class (0=Human, 1=AI)')\n",
    "\n",
    "# Flesch Reading Ease\n",
    "sns.boxplot(data=readability_df, x='generated', y='flesch_reading_ease', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Flesch Reading Ease by Class')\n",
    "axes[0, 1].set_xlabel('Class (0=Human, 1=AI)')\n",
    "\n",
    "# Gunning Fog Index\n",
    "sns.boxplot(data=readability_df, x='generated', y='gunning_fog', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Gunning Fog Index by Class')\n",
    "axes[1, 0].set_xlabel('Class (0=Human, 1=AI)')\n",
    "\n",
    "# Average Sentence Length\n",
    "sns.boxplot(data=readability_df, x='generated', y='avg_sentence_length', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Average Sentence Length by Class')\n",
    "axes[1, 1].set_xlabel('Class (0=Human, 1=AI)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prompt Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distribution across prompts\n",
    "prompt_analysis = train_essays.groupby(['prompt_id', 'generated']).size().unstack(fill_value=0)\n",
    "prompt_analysis['total'] = prompt_analysis.sum(axis=1)\n",
    "prompt_analysis['ai_percentage'] = (prompt_analysis[1] / prompt_analysis['total'] * 100).round(2)\n",
    "\n",
    "print(\"Essays per Prompt and Class:\")\n",
    "print(prompt_analysis)\n",
    "\n",
    "# Merge with prompt names for better visualization\n",
    "prompt_info = train_prompts[['prompt_id', 'prompt_name']]\n",
    "prompt_analysis_named = prompt_analysis.reset_index().merge(prompt_info, on='prompt_id')\n",
    "\n",
    "# Visualize prompt distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Stacked bar chart\n",
    "prompt_analysis_named.set_index('prompt_name')[[0, 1]].plot(kind='bar', stacked=True, \n",
    "                                                           ax=axes[0], color=['skyblue', 'lightcoral'])\n",
    "axes[0].set_title('Essays by Prompt and Class')\n",
    "axes[0].set_xlabel('Prompt')\n",
    "axes[0].set_ylabel('Number of Essays')\n",
    "axes[0].legend(['Human', 'AI'])\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# AI percentage by prompt\n",
    "axes[1].bar(prompt_analysis_named['prompt_name'], prompt_analysis_named['ai_percentage'], \n",
    "           color='lightcoral')\n",
    "axes[1].set_title('AI-Generated Essay Percentage by Prompt')\n",
    "axes[1].set_xlabel('Prompt')\n",
    "axes[1].set_ylabel('AI Percentage (%)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Perform statistical tests to determine if differences are significant\n",
    "human_data = train_essays[train_essays['generated'] == 0]\n",
    "ai_data = train_essays[train_essays['generated'] == 1]\n",
    "\n",
    "# T-tests for continuous variables\n",
    "tests = {\n",
    "    'text_length': stats.ttest_ind(human_data['text_length'], ai_data['text_length']),\n",
    "    'word_count': stats.ttest_ind(human_data['word_count'], ai_data['word_count']),\n",
    "}\n",
    "\n",
    "# Add punctuation tests\n",
    "human_punct = punct_df[punct_df['generated'] == 0]\n",
    "ai_punct = punct_df[punct_df['generated'] == 1]\n",
    "\n",
    "punct_tests = {\n",
    "    'punct_density': stats.ttest_ind(human_punct['punct_density'], ai_punct['punct_density']),\n",
    "    'periods': stats.ttest_ind(human_punct['periods'], ai_punct['periods']),\n",
    "    'commas': stats.ttest_ind(human_punct['commas'], ai_punct['commas']),\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(\"Statistical Significance Tests (p < 0.05 indicates significant difference):\")\n",
    "print(\"\\nText Features:\")\n",
    "for feature, (statistic, p_value) in tests.items():\n",
    "    significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "    print(f\"{feature}: t={statistic:.3f}, p={p_value:.6f} {significance}\")\n",
    "\n",
    "print(\"\\nPunctuation Features:\")\n",
    "for feature, (statistic, p_value) in punct_tests.items():\n",
    "    significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "    print(f\"{feature}: t={statistic:.3f}, p={p_value:.6f} {significance}\")\n",
    "\n",
    "print(\"\\nLegend: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary Statistics and Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive feature dataset\n",
    "features_df = train_essays[['id', 'prompt_id', 'generated', 'text_length', 'word_count', 'sentence_count']].copy()\n",
    "features_df = features_df.merge(punct_df[['periods', 'commas', 'exclamations', 'questions', \n",
    "                                        'punct_density', 'generated']], on='generated')\n",
    "\n",
    "# Calculate correlation matrix\n",
    "feature_cols = ['text_length', 'word_count', 'sentence_count', 'periods', 'commas', \n",
    "               'exclamations', 'questions', 'punct_density']\n",
    "correlation_matrix = features_df[feature_cols + ['generated']].corr()\n",
    "\n",
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "           square=True, fmt='.3f')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show correlations with target variable\n",
    "target_correlations = correlation_matrix['generated'].drop('generated').abs().sort_values(ascending=False)\n",
    "print(\"\\nFeatures ranked by absolute correlation with target (generated):\")\n",
    "for feature, corr in target_correlations.items():\n",
    "    print(f\"{feature}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Findings and Initial Hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key findings\n",
    "print(\"=== KEY FINDINGS FROM EXPLORATORY DATA ANALYSIS ===\")\n",
    "print()\n",
    "\n",
    "print(\"1. CLASS BALANCE:\")\n",
    "print(f\"   - Human essays: {class_counts[0]} ({class_proportions[0]:.2%})\")\n",
    "print(f\"   - AI essays: {class_counts[1]} ({class_proportions[1]:.2%})\")\n",
    "print(f\"   - Dataset is heavily imbalanced towards human essays\")\n",
    "print()\n",
    "\n",
    "print(\"2. TEXT LENGTH PATTERNS:\")\n",
    "human_avg_length = train_essays[train_essays['generated'] == 0]['text_length'].mean()\n",
    "ai_avg_length = train_essays[train_essays['generated'] == 1]['text_length'].mean()\n",
    "print(f\"   - Human essays average length: {human_avg_length:.0f} characters\")\n",
    "print(f\"   - AI essays average length: {ai_avg_length:.0f} characters\")\n",
    "print(f\"   - Difference: {abs(human_avg_length - ai_avg_length):.0f} characters\")\n",
    "print()\n",
    "\n",
    "print(\"3. WORD COUNT PATTERNS:\")\n",
    "human_avg_words = train_essays[train_essays['generated'] == 0]['word_count'].mean()\n",
    "ai_avg_words = train_essays[train_essays['generated'] == 1]['word_count'].mean()\n",
    "print(f\"   - Human essays average word count: {human_avg_words:.0f} words\")\n",
    "print(f\"   - AI essays average word count: {ai_avg_words:.0f} words\")\n",
    "print(f\"   - Difference: {abs(human_avg_words - ai_avg_words):.0f} words\")\n",
    "print()\n",
    "\n",
    "print(\"4. PUNCTUATION USAGE:\")\n",
    "human_punct_density = punct_df[punct_df['generated'] == 0]['punct_density'].mean()\n",
    "ai_punct_density = punct_df[punct_df['generated'] == 1]['punct_density'].mean()\n",
    "print(f\"   - Human essays punctuation density: {human_punct_density:.2f}%\")\n",
    "print(f\"   - AI essays punctuation density: {ai_punct_density:.2f}%\")\n",
    "print(f\"   - Difference: {abs(human_punct_density - ai_punct_density):.2f}%\")\n",
    "print()\n",
    "\n",
    "print(\"5. MOST DISCRIMINATIVE FEATURES:\")\n",
    "for i, (feature, corr) in enumerate(target_correlations.head(5).items(), 1):\n",
    "    print(f\"   {i}. {feature}: {corr:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"=== INITIAL HYPOTHESES FOR MODEL DEVELOPMENT ===\")\n",
    "print()\n",
    "print(\"1. FEATURE ENGINEERING OPPORTUNITIES:\")\n",
    "print(\"   - Text length and word count appear to be discriminative\")\n",
    "print(\"   - Punctuation patterns may help distinguish AI from human text\")\n",
    "print(\"   - Readability scores could capture complexity differences\")\n",
    "print(\"   - N-gram analysis might reveal linguistic patterns\")\n",
    "print()\n",
    "print(\"2. MODELING CONSIDERATIONS:\")\n",
    "print(\"   - Class imbalance needs to be addressed (sampling or class weights)\")\n",
    "print(\"   - Prompt-specific features might be valuable\")\n",
    "print(\"   - Ensemble methods could combine different feature types\")\n",
    "print(\"   - Cross-validation should stratify by both class and prompt\")\n",
    "print()\n",
    "print(\"3. DATA AUGMENTATION POSSIBILITIES:\")\n",
    "print(\"   - Generate more AI essays using different LLMs\")\n",
    "print(\"   - Vary prompt engineering techniques\")\n",
    "print(\"   - Consider synthetic minority oversampling techniques\")\n",
    "\n",
    "print(\"\\n=== NEXT STEPS ===\")\n",
    "print(\"1. Implement advanced feature engineering pipeline\")\n",
    "print(\"2. Address class imbalance\")\n",
    "print(\"3. Develop baseline models\")\n",
    "print(\"4. Set up experiment tracking with MLflow\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}